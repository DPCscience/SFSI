% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lars.R
\name{lars2}
\alias{lars2}
\title{Least Angle Regression to solve the LASSO-type problem}
\usage{
lars2(XtX, Xty, method = c("LAR", "LAR-LASSO"), maxDF = NULL,
  eps = .Machine$double.eps, scale = TRUE, verbose = FALSE)
}
\arguments{
\item{XtX}{Variance-covariance matrix among predictors}

\item{Xty}{Covariance vector between response variable and predictors}

\item{method}{One of:
\itemize{
 \item \code{'LAR'}: Computes the entire sequence of all coefficients. Values of lambdas are calculated at each step.
 \item \code{'LAR-LASSO'}: Similar to \code{'LAR'} but solutions when a predictor leaves the solution are also returned.
}
Default is \code{method='LAR'}}

\item{maxDF}{Maximum number of predictors in the last lars solution.
Default \code{maxDF=NULL} will calculate solution for all the predictors}

\item{eps}{An effective zero. Default is the machine precision}

\item{scale}{\code{TRUE} or \code{FALSE} to recalculate the matrix \code{XtX} for variables with unit variance 
(see \code{help(scale_crossprod)}) and scale \code{Xty} by the standard deviation of the corresponding predictor
taken from the diagonal of \code{XtX}}

\item{verbose}{\code{TRUE} or \code{FALSE} to whether printing each lars step}
}
\value{
List with the following elements:
\itemize{
  \item \code{beta}: vector of regression coefficients.
  \item \code{lambda}: penalty of LASSO-type problem for all the sequence of coefficients.
  \item \code{df}: degrees of freedom, number of non-zero predictors at each solution.
  \item \code{sdx}: vector of standard deviation of predictors.
}
}
\description{
Computes the entire LASSO solution for the regression coefficients, starting from zero, to the
least squares estimates, via the Least Angle Regression (LARS) algorithm (Efron, 2004). It uses as inputs
a variance matrix among predictors and a covariance vector between response and predictors.
}
\details{
The regression coefficients \eqn{\beta=(\beta_1,...,\beta_p)'} are estimated as function of the variance matrix among
predictors (\eqn{XtX}) and the covariance vector between response and predictors (\eqn{Xty}) using 'covariance updates'
to solve the optimization function
\deqn{-Xty' \beta + 1/2\beta' (XtX)\beta + 1/2\lambda||\beta||_2^2}
where \eqn{\lambda} is the penalization parameter
}
\examples{
set.seed(1234)
require(SFSI)
# Simulate variables
n = 500; p=200;  rho=0.65
X = matrix(rnorm(n*p),ncol=p)
eta = scale(X\%*\%rnorm(p))  # signal
e =  rnorm(n)              # noise
y = rho*eta + sqrt(1-rho^2)*e

# Training and testing sets
pTST = 0.3      # percentage to predict
tst = sample(1:n,floor(pTST*n))
trn = (1:n)[-tst]

# Calculate covariances in training set
P = var(X[trn,])
rhs = as.vector(cov(y[trn],X[trn,]))

# Run the penalized regression
fm = lars2(P,rhs,method="LAR-LASSO",verbose=TRUE)

# Regression coefficients
beta = as.matrix(fm$beta)

# Predicted values in training and testing set
yHat_TRN =  X[trn,] \%*\% t(beta)
yHat_TST =  X[tst,] \%*\% t(beta)

par(mfrow=c(1,2))
plot(fm$df,cor(y[trn],yHat_TRN)[1,],main="Training set")
plot(fm$df,cor(y[tst],yHat_TST)[1,],main="Testing set")
}
\references{
\itemize{
\item \insertRef{Efron2004}{SFSI}
\item \insertRef{Friedman2010}{SFSI}
\item \insertRef{Hastie2013}{SFSI}
\item \insertRef{Tibshirani1996}{SFSI}
}
}
\author{
Marco Lopez-Cruz (\email{lopezcru@msu.edu}) and Gustavo de los Campos. Adapted from 'lars' package (Hastie & Efron, 2013)
}
\keyword{lars2}
